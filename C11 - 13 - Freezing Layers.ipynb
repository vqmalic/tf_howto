{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have been reusing layers from older, trained models, but they *still* are updated during the new round of training. They're probably near where they \"should\" be, assuming the new task is similar to the old one, but the gradients for these lower, reused layers are still calculated and the weights are still updated. For a very large neural network, it's a good idea to take \"reuse\" to its logical conclusion and freeze this weights under the confidence that they're \"already where they should be.\" For large networks in particular, this will speed up training considerably since there are far fewer parameters to train. Freezing layers is implemented here but there won't be noticeable gains in such a small network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "# load data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construction\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # isolating only hidden3, hidden4, and outputs as trainable\n",
    "    # hidden1 and hidden2 are frozen\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "    # pass trainable variables to the optimizer when making training op\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver() # need to make a new saver, if using another one to load existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from savedmodels/11_07_gradientclipping.ckpt\n",
      "0 train acc: 0.896855 val acc: 0.8986\n",
      "5 train acc: 0.953073 val acc: 0.95\n",
      "10 train acc: 0.958582 val acc: 0.956\n",
      "15 train acc: 0.960509 val acc: 0.958\n",
      "19 train acc: 0.961727 val acc: 0.9598\n",
      "Test acc: 0.9567\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# at this point, certain layers are frozen\n",
    "# still need to load the pretrained weights\n",
    "\n",
    "# reuse 1, 2, and 3. 1 and 2 are frozen, 3 will be re-trained.\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # pulls old weights in\n",
    "    restore_saver.restore(sess, \"savedmodels/11_07_gradientclipping.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            acc_train = accuracy.eval(feed_dict={X:mnist.train.images, y:mnist.train.labels})\n",
    "            acc_val = accuracy.eval(feed_dict={X:mnist.validation.images, y:mnist.validation.labels})\n",
    "            print(epoch, \"train acc:\", acc_train, \"val acc:\", acc_val)\n",
    "    acc_test = accuracy.eval(feed_dict={X:mnist.test.images, y:mnist.test.labels})\n",
    "    print(\"Test acc:\", acc_test)\n",
    "    save_path = new_saver.save(sess, \"savedmodels/11_13_frozen.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for self: if I restore the checkpoint I just made, are the frozen layers still frozen?\n",
    "\n",
    "Answer: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from savedmodels/11_13_frozen.ckpt\n",
      "**************************************************\n",
      "Epoch 0\n",
      "0 Test accuracy: 0.9018\n",
      "**************************************************\n",
      "Epoch 1\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "1.19209e-06\n",
      "1 Test accuracy: 0.9353\n",
      "**************************************************\n",
      "Epoch 2\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "5.88596e-07\n",
      "2 Test accuracy: 0.9421\n",
      "**************************************************\n",
      "Epoch 3\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-1.85519e-06\n",
      "3 Test accuracy: 0.9457\n",
      "**************************************************\n",
      "Epoch 4\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "2.99886e-07\n",
      "4 Test accuracy: 0.949\n",
      "**************************************************\n",
      "Epoch 5\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "1.89431e-06\n",
      "5 Test accuracy: 0.9492\n",
      "**************************************************\n",
      "Epoch 6\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-9.94653e-07\n",
      "6 Test accuracy: 0.9507\n",
      "**************************************************\n",
      "Epoch 7\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-2.11876e-06\n",
      "7 Test accuracy: 0.9518\n",
      "**************************************************\n",
      "Epoch 8\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-1.07661e-06\n",
      "8 Test accuracy: 0.9521\n",
      "**************************************************\n",
      "Epoch 9\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "1.63913e-07\n",
      "9 Test accuracy: 0.9515\n",
      "**************************************************\n",
      "Epoch 10\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "3.74578e-06\n",
      "10 Test accuracy: 0.9517\n",
      "**************************************************\n",
      "Epoch 11\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "9.08971e-07\n",
      "11 Test accuracy: 0.9538\n",
      "**************************************************\n",
      "Epoch 12\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "7.82311e-07\n",
      "12 Test accuracy: 0.9541\n",
      "**************************************************\n",
      "Epoch 13\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "1.88593e-06\n",
      "13 Test accuracy: 0.9537\n",
      "**************************************************\n",
      "Epoch 14\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-2.32831e-08\n",
      "14 Test accuracy: 0.953\n",
      "**************************************************\n",
      "Epoch 15\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "4.73112e-07\n",
      "15 Test accuracy: 0.9557\n",
      "**************************************************\n",
      "Epoch 16\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-1.68942e-06\n",
      "16 Test accuracy: 0.954\n",
      "**************************************************\n",
      "Epoch 17\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "2.61702e-07\n",
      "17 Test accuracy: 0.9554\n",
      "**************************************************\n",
      "Epoch 18\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "1.67498e-06\n",
      "18 Test accuracy: 0.9562\n",
      "**************************************************\n",
      "Epoch 19\n",
      "h1 diff\n",
      "0.0\n",
      "h4 diff\n",
      "-2.21655e-07\n",
      "19 Test accuracy: 0.9545\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"savedmodels/11_13_frozen.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"*\"*50)\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        h1 = sess.run(\"hidden1/kernel:0\")\n",
    "        h4 = sess.run(\"outputs/kernel:0\")\n",
    "        if \"h1_old\" in locals():\n",
    "            print(\"h1 diff\")\n",
    "            print(np.sum(h1-h1_old))\n",
    "        if \"h4_old\" in locals():\n",
    "            print(\"h4 diff\")\n",
    "            print(np.sum(h4-h4_old))\n",
    "        h1_old = h1\n",
    "        h4_old = h4\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
