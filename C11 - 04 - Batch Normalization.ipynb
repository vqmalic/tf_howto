{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization is another common approach to the vanishing/exploding gradient problem. Again, the trick is to somehow ensure that the outputs of neurons in a given layer are centered on mean 0 and have a standard deviation of 1. Batch Normalization also adds two additional, learned parameters for the scaling and shifting of the outputs, once they've been normalized.\n",
    "\n",
    "Means and standard deviations are calculated by mini-batch, hence \"Batch\" normalization. During testing or deployment, means and stds are calculated from the entirety of the training data. Behavior is different during training and testing, and so the network needs to know which its doing. It's computationally efficient to calculate the mean and std for *all* the training data - which is needed during testing - *during* training. TensorFlow does this by adding new operations for keeping a running mean and std during training. It's important to be aware of this - these ops need to be called during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# decay for calculation of running averages\n",
    "# more 9s for bigger datasets\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "# Batch normalization acts different if you're training or deploying\n",
    "# We make a boolean variable to indicate whether or not we're training\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # no activation in dense layer\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    # instead, after dense, apply BN, then intended activation\n",
    "    bn1 = tf.nn.elu(tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum))\n",
    "    hidden2 = tf.layers.dense(bn1, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum))\n",
    "    logits_before_bn = tf.layers.dense(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum, name=\"logits\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y , 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train acc: 0.9162 val acc: 0.9152\n",
      "5 train acc: 0.971091 val acc: 0.9688\n",
      "10 train acc: 0.984273 val acc: 0.9772\n",
      "15 train acc: 0.990546 val acc: 0.9786\n",
      "20 train acc: 0.993237 val acc: 0.981\n",
      "25 train acc: 0.996 val acc: 0.9812\n",
      "30 train acc: 0.996582 val acc: 0.9778\n",
      "35 train acc: 0.997964 val acc: 0.9798\n",
      "39 train acc: 0.998709 val acc: 0.9822\n",
      "Test acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "# execution\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# here are the running average ops!\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # both training_ops AND extra_update_ops are evaluated\n",
    "            # feed_dict now gets \"is training\" boolean as input\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X:X_batch, y:y_batch})\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            acc_train = accuracy.eval(feed_dict={X:mnist.train.images, y:mnist.train.labels})\n",
    "            acc_val = accuracy.eval(feed_dict={X:mnist.validation.images, y:mnist.validation.labels})\n",
    "            print(epoch, \"train acc:\", acc_train, \"val acc:\", acc_val)\n",
    "    # Now we're testing\n",
    "    # We set up the training boolean to have a default value of False\n",
    "    # therefore we don't need to pass it in the feed dict\n",
    "    # batch normalization will be calculated accordingly\n",
    "    acc_test = accuracy.eval(feed_dict={X:mnist.test.images, y:mnist.test.labels})\n",
    "    print(\"Test acc:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
