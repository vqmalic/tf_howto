{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a pretrained model, but we want to train it on a different, but similar task. The weights on the pretrained model should be about where they \"need to be\" for the different, similar task, so starting with them would be helpful. However, this applies mostly to the lower layers, which capture low-level features, while the higher layers may actually be *too* specialized for the new task. \n",
    "\n",
    "In this case, we want to use the lower layers of the pretrained model but use new upper layers with randomly initialized weights. One way is to load the entire pretrained model, but at the point where you want brand new layers, make a branching *new set of layers* that take the output of the last pretrained layer you want to use as input. \n",
    "\n",
    "The upper layers of the old graph are still there - they're just not being used, which is why I call this the \"branching\" or \"Siamese twin\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"savedmodels/11_07_gradientclipping.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "# We want new layers after hidden3\n",
    "# We get the final activations from hidden3\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "# Then, we continue building as normal - using \"new\" to avoid name collisions\n",
    "\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_hidden5 = tf.layers.dense(new_hidden4, n_hidden5, activation=tf.nn.relu, name=\"new_hidden5\")\n",
    "new_logits = tf.layers.dense(new_hidden5, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    train_summary = tf.summary.scalar('train_accuracy', accuracy)\n",
    "    valid_summary = tf.summary.scalar('valid_accuracy', accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(\"to_tensorboard/11_10_branching\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from savedmodels/11_07_gradientclipping.ckpt\n",
      "0 train acc: 0.920455 val acc: 0.9252\n",
      "5 train acc: 0.956782 val acc: 0.9582\n",
      "10 train acc: 0.966455 val acc: 0.9642\n",
      "15 train acc: 0.973945 val acc: 0.9686\n",
      "19 train acc: 0.977236 val acc: 0.9694\n",
      "Test acc: 0.9666\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # restore weights from old run\n",
    "    # all weights will be restored, but \"new\" prefixed variables\n",
    "    # will be initialized in the default manner\n",
    "    saver.restore(sess, \"savedmodels/11_07_gradientclipping.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train, tra_str = sess.run([accuracy, train_summary], feed_dict={X:mnist.train.images, y:mnist.train.labels})\n",
    "        acc_val, val_str = sess.run([accuracy, valid_summary], feed_dict={X:mnist.validation.images, y:mnist.validation.labels})\n",
    "        file_writer.add_summary(tra_str, epoch)\n",
    "        file_writer.add_summary(val_str, epoch)\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            print(epoch, \"train acc:\", acc_train, \"val acc:\", acc_val)\n",
    "    acc_test = accuracy.eval(feed_dict={X:mnist.test.images, y:mnist.test.labels})\n",
    "    print(\"Test acc:\", acc_test)\n",
    "    \n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
